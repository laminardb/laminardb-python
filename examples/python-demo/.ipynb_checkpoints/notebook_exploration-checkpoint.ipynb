{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LaminarDB Interactive Exploration\n",
        "\n",
        "A Jupyter-native walkthrough of LaminarDB's Python API.  \n",
        "Think of it as a **streaming SQL engine** you can `pip install`.\n",
        "\n",
        "```bash\n",
        "pip install laminardb pandas polars plotly\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import laminardb\n",
        "from laminardb import ChangeEvent\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(f\"LaminarDB {laminardb.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Connect & Query\n",
        "\n",
        "Embedded database -- no server, no Docker, no config files.  \n",
        "Inline SQL queries work out of the box via DataFusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = laminardb.open(\":memory:\")\n",
        "\n",
        "# Inline SQL -- renders as an HTML table in Jupyter (_repr_html_)\n",
        "db.sql(\"SELECT 42 AS answer, 'hello' AS greeting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Sources & Load Data\n",
        "\n",
        "Sources are streaming ingestion points. LaminarDB accepts  \n",
        "**pandas DataFrames**, **polars DataFrames**, **PyArrow Tables**,  \n",
        "**dicts**, **JSON strings**, and **CSV strings**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a source (streaming ingestion point)\n",
        "db.execute(\"\"\"\n",
        "    CREATE SOURCE market_data (\n",
        "        symbol  VARCHAR,\n",
        "        price   DOUBLE,\n",
        "        volume  BIGINT,\n",
        "        ts      BIGINT\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# Generate 10k rows of sample data\n",
        "np.random.seed(42)\n",
        "n = 10_000\n",
        "symbols = np.random.choice([\"AAPL\", \"GOOG\", \"MSFT\", \"NVDA\"], n)\n",
        "base_prices = {\"AAPL\": 185, \"GOOG\": 175, \"MSFT\": 415, \"NVDA\": 875}\n",
        "\n",
        "sample = pd.DataFrame({\n",
        "    \"symbol\": symbols,\n",
        "    \"price\": [base_prices[s] * np.exp(np.random.normal(0, 0.02)) for s in symbols],\n",
        "    \"volume\": np.random.randint(100, 100_000, n),\n",
        "    \"ts\": np.arange(n),\n",
        "})\n",
        "\n",
        "# Insert the entire DataFrame -- LaminarDB handles the conversion\n",
        "rows = db.insert(\"market_data\", sample)\n",
        "print(f\"Inserted {rows:,} rows\")\n",
        "sample.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Streaming Queries -- The LaminarDB Difference\n",
        "\n",
        "Create **continuous queries** that filter or transform data in real time.  \n",
        "Data flows: `source -> stream -> subscription`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stream 1: Only high-volume trades (> 80,000 shares)\n",
        "db.execute(\"\"\"\n",
        "    CREATE STREAM whale_trades AS\n",
        "    SELECT symbol, price, volume, ts\n",
        "    FROM market_data\n",
        "    WHERE volume > 80000\n",
        "\"\"\")\n",
        "\n",
        "# Stream 2: All trades pass through (for analytics collection)\n",
        "db.execute(\"\"\"\n",
        "    CREATE STREAM all_trades AS\n",
        "    SELECT symbol, price, volume, ts\n",
        "    FROM market_data\n",
        "\"\"\")\n",
        "\n",
        "# Start the streaming pipeline\n",
        "db.start()\n",
        "print(\"Pipeline started!\")\n",
        "print(f\"Sources: {db.tables()}\")\n",
        "print(f\"Streams: {db.materialized_views()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Real-Time Subscriptions\n",
        "\n",
        "Subscribe to streaming data. New data is pushed to your callback in real time.  \n",
        "No polling. No batch queries. Just events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect whale trades and all trades via subscriptions\n",
        "whale_data = []\n",
        "all_data = []\n",
        "\n",
        "mv_whale = laminardb.mv(db, \"whale_trades\")\n",
        "mv_all = laminardb.mv(db, \"all_trades\")\n",
        "\n",
        "def on_whale(event: ChangeEvent):\n",
        "    for row in event:\n",
        "        whale_data.append(row.to_dict())\n",
        "\n",
        "def on_all(event: ChangeEvent):\n",
        "    for row in event:\n",
        "        all_data.append(row.to_dict())\n",
        "\n",
        "t1 = mv_whale.subscribe(handler=on_whale)\n",
        "t2 = mv_all.subscribe(handler=on_all)\n",
        "\n",
        "# Insert more data and let subscriptions collect\n",
        "for i in range(5):\n",
        "    batch = pd.DataFrame({\n",
        "        \"symbol\": np.random.choice([\"AAPL\", \"GOOG\", \"MSFT\", \"NVDA\"], 200),\n",
        "        \"price\": np.random.lognormal(5, 0.3, 200),\n",
        "        \"volume\": np.random.randint(100, 100_000, 200),\n",
        "        \"ts\": np.arange(n + i * 200, n + (i + 1) * 200),\n",
        "    })\n",
        "    db.insert(\"market_data\", batch)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "time.sleep(0.5)  # Let subscriptions drain\n",
        "\n",
        "print(f\"Collected {len(all_data):,} trades via all_trades stream\")\n",
        "print(f\"Collected {len(whale_data)} whale trades (volume > 80k)\")\n",
        "\n",
        "# Show some whale trades\n",
        "if whale_data:\n",
        "    print(\"\\nSample whale trades:\")\n",
        "    for t in whale_data[:5]:\n",
        "        print(f\"  {t['symbol']:>5s}  ${t['price']:.2f}  vol={t['volume']:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Multi-Format Output\n",
        "\n",
        "Query results and subscription data can be exported to pandas, polars,  \n",
        "PyArrow, or plain dicts. Inline SQL works for ad-hoc queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inline SQL with multiple output formats\n",
        "result = db.sql(\"\"\"\n",
        "    SELECT * FROM (VALUES\n",
        "        ('AAPL', 185.50, 12000),\n",
        "        ('GOOG', 175.20, 8000),\n",
        "        ('MSFT', 415.00, 35000),\n",
        "        ('NVDA', 875.00, 42000)\n",
        "    ) AS t(symbol, price, volume)\n",
        "\"\"\")\n",
        "\n",
        "print(\"=== .fetchall() ===\")\n",
        "print(result.fetchall())\n",
        "\n",
        "print(\"\\n=== .to_dicts() ===\")\n",
        "print(result.to_dicts())\n",
        "\n",
        "print(f\"\\nlen={len(result)}, columns={result.columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .df() -> pandas DataFrame\n",
        "print(\"=== .df() -> pandas ===\")\n",
        "display(result.df())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .pl() -> polars DataFrame\n",
        "try:\n",
        "    import polars as pl\n",
        "\n",
        "    print(\"=== .pl() -> polars ===\")\n",
        "    display(result.pl())\n",
        "\n",
        "    # Polars lazy evaluation on subscription data\n",
        "    if all_data:\n",
        "        print(\"\\n=== Polars lazy group_by on subscription data ===\")\n",
        "        lazy_result = (\n",
        "            pl.DataFrame(all_data)\n",
        "            .lazy()\n",
        "            .filter(pl.col(\"volume\") > 50_000)\n",
        "            .group_by(\"symbol\")\n",
        "            .agg([\n",
        "                pl.col(\"price\").mean().alias(\"avg_price\"),\n",
        "                pl.col(\"volume\").sum().alias(\"total_volume\"),\n",
        "                pl.col(\"volume\").count().alias(\"count\"),\n",
        "            ])\n",
        "            .sort(\"total_volume\", descending=True)\n",
        "            .collect()\n",
        "        )\n",
        "        display(lazy_result)\n",
        "except ImportError:\n",
        "    print(\"(polars not installed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .arrow() -> PyArrow Table\n",
        "try:\n",
        "    arrow_table = result.arrow()\n",
        "    print(\"=== .arrow() -> PyArrow Table ===\")\n",
        "    print(arrow_table)\n",
        "    print(f\"\\nSchema: {arrow_table.schema}\")\n",
        "except ImportError:\n",
        "    print(\"(pyarrow not installed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Plotly Visualization\n",
        "\n",
        "Visualize subscription-collected data with Plotly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import plotly.express as px\n",
        "\n",
        "    if all_data:\n",
        "        price_df = pd.DataFrame(all_data)\n",
        "\n",
        "        fig = px.histogram(\n",
        "            price_df,\n",
        "            x=\"price\",\n",
        "            color=\"symbol\",\n",
        "            nbins=60,\n",
        "            title=\"Price Distribution by Symbol (from streaming subscription)\",\n",
        "            barmode=\"overlay\",\n",
        "            opacity=0.7,\n",
        "        )\n",
        "        fig.update_layout(xaxis_title=\"Price ($)\", yaxis_title=\"Count\")\n",
        "        fig.show()\n",
        "    else:\n",
        "        print(\"No data collected yet\")\n",
        "except ImportError:\n",
        "    print(\"(plotly not installed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import plotly.express as px\n",
        "\n",
        "    if all_data:\n",
        "        df = pd.DataFrame(all_data)\n",
        "        # Volume by symbol\n",
        "        vol_summary = df.groupby(\"symbol\").agg(\n",
        "            total_volume=(\"volume\", \"sum\"),\n",
        "            avg_price=(\"price\", \"mean\"),\n",
        "            trade_count=(\"volume\", \"count\"),\n",
        "        ).reset_index()\n",
        "\n",
        "        fig = px.bar(\n",
        "            vol_summary,\n",
        "            x=\"symbol\",\n",
        "            y=\"total_volume\",\n",
        "            color=\"symbol\",\n",
        "            title=\"Total Volume by Symbol (streaming data)\",\n",
        "        )\n",
        "        fig.update_layout(yaxis_title=\"Total Volume\")\n",
        "        fig.show()\n",
        "except ImportError:\n",
        "    print(\"(plotly not installed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Schema Introspection\n",
        "\n",
        "Explore your database catalog -- sources, streams, schemas, and statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Sources (tables):       {db.tables()}\")\n",
        "print(f\"Streams (materialized): {db.materialized_views()}\")\n",
        "print(f\"\\nSchema for 'market_data':\")\n",
        "print(db.schema(\"market_data\"))\n",
        "\n",
        "print(f\"\\nTable stats:\")\n",
        "print(db.stats(\"market_data\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed catalog info\n",
        "print(\"=== Sources ===\")\n",
        "for src in db.sources():\n",
        "    print(f\"  {src.name}: schema={src.schema}, watermark_col={src.watermark_column}\")\n",
        "\n",
        "print(\"\\n=== Streams ===\")\n",
        "for stream in db.streams():\n",
        "    print(f\"  {stream.name}: sql={stream.sql}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Pipeline Metrics & Topology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = db.metrics()\n",
        "print(f\"Pipeline state:    {m.state}\")\n",
        "print(f\"Events ingested:   {m.total_events_ingested:,}\")\n",
        "print(f\"Uptime:            {m.uptime_secs:.1f}s\")\n",
        "\n",
        "print(\"\\n--- Source Metrics ---\")\n",
        "for sm in db.all_source_metrics():\n",
        "    print(f\"  {sm.name}: events={sm.total_events:,}, pending={sm.pending}, watermark={sm.watermark}\")\n",
        "\n",
        "print(\"\\n--- Topology ---\")\n",
        "topo = db.topology()\n",
        "for node in topo.nodes:\n",
        "    print(f\"  [{node.node_type}] {node.name}\")\n",
        "for edge in topo.edges:\n",
        "    print(f\"    {edge.from_node} --> {edge.to_node}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Query Plan\n",
        "\n",
        "Understand how LaminarDB executes your SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan = db.explain(\"SELECT 1 + 1 AS answer\")\n",
        "print(plan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Writer API -- Streaming Inserts\n",
        "\n",
        "For high-throughput ingestion, use the Writer context manager."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "with db.writer(\"market_data\") as w:\n",
        "    for i in range(10):\n",
        "        batch = [\n",
        "            {\n",
        "                \"symbol\": random.choice([\"AAPL\", \"GOOG\", \"MSFT\", \"NVDA\"]),\n",
        "                \"price\": round(random.uniform(100, 900), 2),\n",
        "                \"volume\": random.randint(100, 50_000),\n",
        "                \"ts\": 300_000 + i * 100 + j,\n",
        "            }\n",
        "            for j in range(100)\n",
        "        ]\n",
        "        w.insert(batch)\n",
        "\n",
        "    print(f\"Writer source: {w.name}\")\n",
        "    print(f\"Writer schema: {w.schema}\")\n",
        "    print(f\"Watermark: {w.current_watermark}\")\n",
        "\n",
        "# Let subscriptions pick up the new data\n",
        "time.sleep(0.5)\n",
        "print(f\"\\nTotal trades collected via subscription: {len(all_data):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "LaminarDB is a **streaming-first** database:\n",
        "\n",
        "| Concept | How It Works |\n",
        "|---|---|\n",
        "| **Sources** | Ingestion points (`CREATE SOURCE`, `insert()`, `writer()`) |\n",
        "| **Streams** | Continuous SQL queries (`CREATE STREAM ... AS SELECT ... FROM source`) |\n",
        "| **Subscriptions** | Real-time delivery (`mv.subscribe(handler=)`) |\n",
        "| **Inline SQL** | Ad-hoc queries via DataFusion (`db.sql(\"SELECT ...\")`) |\n",
        "\n",
        "Data flows through the pipeline: **source -> stream -> subscription**.  \n",
        "No Kafka. No Flink. Just `pip install laminardb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db.close()\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbformat_minor": 4,
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
